import os
import requests
from datetime import datetime, timezone, timedelta

# 定义源目录和输出目录
SOURCES_DIR = 'sources'
RULES_DIR = 'rules'

def get_beijing_time():
    """获取北京时间"""
    utc_now = datetime.now(timezone.utc)
    beijing_tz = timezone(timedelta(hours=8))
    return utc_now.astimezone(beijing_tz)

def process_rules():
    """
    主处理函数：读取源文件，获取、去重并写入规则。
    新版：能智能处理 .mrs 二进制文件和 .list 文本文件。
    """
    # 确保输出目录存在
    if not os.path.exists(RULES_DIR):
        os.makedirs(RULES_DIR)

    # 遍历源目录下的所有 .txt 文件
    for source_file in os.listdir(SOURCES_DIR):
        if not source_file.endswith('.txt'):
            continue

        # 根据源文件名确定输出文件名
        output_filename = os.path.splitext(source_file)[0] + '.list'
        output_path = os.path.join(RULES_DIR, output_filename)
        
        source_path = os.path.join(SOURCES_DIR, source_file)
        print(f"[*] Processing {source_path} -> {output_path}")

        # 读取源文件中的所有 URL
        with open(source_path, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]

        # 检查源 URL 是否包含 .mrs，以决定处理方式
        is_binary_mrs = any('.mrs' in url for url in urls)

        if is_binary_mrs:
            # --- 二进制 .mrs 文件处理方式 ---
            print(f"  [i] Detected .mrs source. Processing as binary.")
            if len(urls) > 1:
                print(f"  [!] Warning: Multiple URLs found for a .mrs rule set. Only the first one will be used.")
            
            url = urls[0]
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                # 直接将二进制内容写入文件
                with open(output_path, 'wb') as f_out:
                    f_out.write(response.content)
                print(f"  [+] Successfully wrote binary content from {url}")
            except requests.RequestException as e:
                print(f"  [!] Failed to fetch {url}: {e}")

        else:
            # --- 纯文本 .list/.txt 文件处理方式 ---
            print(f"  [i] Processing as plain text.")
            unique_rules = set()
            for url in urls:
                try:
                    response = requests.get(url, timeout=30)
                    response.raise_for_status()
                    
                    lines = response.text.split('\n')
                    rule_count = 0
                    for line in lines:
                        line = line.split('#')[0].strip()
                        if line:
                            unique_rules.add(line)
                            rule_count += 1
                    print(f"  [+] Fetched {rule_count} text rules from {url}")
                except requests.RequestException as e:
                    print(f"  [!] Failed to fetch {url}: {e}")

            if unique_rules:
                sorted_rules = sorted(list(unique_rules))
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(f"# RuleSet for Clash - {output_filename}\n")
                    f.write(f"# Generated by Gemini & GitHub Actions\n")
                    f.write(f"# Last Updated: {get_beijing_time().strftime('%Y-%m-%d %H:%M:%S CST')}\n")
                    f.write(f"# Total Rules: {len(sorted_rules)}\n\n")
                    f.write('\n'.join(sorted_rules))
                print(f"[*] Successfully wrote {len(sorted_rules)} unique text rules to {output_path}\n")


if __name__ == '__main__':
    process_rules()
